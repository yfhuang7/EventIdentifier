{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f184eb58-a038-41b9-bf94-93144cb58e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hydrological event identification based on hydrograph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fee96394-b474-4331-922d-c3488f7b6963",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import packages\n",
    "# general packages\n",
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# packages for speical usage\n",
    "from hydrotools.nwis_client import IVDataService\n",
    "\n",
    "import itertools\n",
    "from pandas import Grouper\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f776b88b-5485-48ba-b421-55fa755ae6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "startDate = \"2020-01-01\"\n",
    "endDate = \"2020-12-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e587e432-3bbb-4a84-9121-febbacd5fe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Directory and file paths\n",
    "# os.chdir(\"~/Dropbox/PhD_Dissertation/CH3_WRFHydro_project/Script/\")    # set current working directory\n",
    "\n",
    "# # directories\n",
    "# dir_result = \"../Results/event_identification/\"\n",
    "\n",
    "# files\n",
    "file_site_meta = \"./Data/domainMeta_detail.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7950c0a-f3c1-4653-bae3-8cda902afd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Conversions\n",
    "cfs2cms = 0.02832\n",
    "sqmi2sqkm = 2.58999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "042f3dd0-57ca-4f29-b5c0-42dc202b36f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_identify(obs_gage, dir_output, gage_id, thrld_min, thrld_jump=80, thrld_flat=75, no_flat=3, thrld_duration='6H',\n",
    "                  plot = True, plot_start=None, plot_end=None):\n",
    "    '''\n",
    "    Identify events based on the hydrograph (raising and falling limb identification) + applied thresholds.\n",
    "    obs_gage: hourly streamflow time series with `datetime` (UTC) as the index and `value` as the streamflow (cms).\n",
    "    dir_output: output directory.\n",
    "    gage_id: string. for creating the output file.\n",
    "    thrld_min: minimum peak streamflow (cms) to consider an event as identified event.\n",
    "    thrld_jump: percentile of changes/shifts (in an hour) to identify a \"jump\" (to prevent tiny jumps from unstable/sensitive streamflow). Default: 80.\n",
    "    thrld_flat: percentile of changes/shifts (in an hour) to identify \"flat\" area.\n",
    "    no_flat: the number of timesteps for consecutively counting as 'flat'.\n",
    "    thrld_duration: the threshold for the event duration. Default - at least longer than 6H.\n",
    "    plot: if you want a plot or not\n",
    "    plot_start: the starting time of the plot. default is the begining of the data\n",
    "    plot_end: the ending time of the plot. default is the last data point.\n",
    "    '''\n",
    "    # determine the \"jump\"\n",
    "    data = obs_gage.value-obs_gage.value.shift(1)\n",
    "    obs_gage.loc[:,'jump'] = data>np.percentile([x for x in data if x>0], thrld_jump)  # need to have this to prevent getting tiny jump from unstable streamflow\n",
    "\n",
    "    ## Looking for the \"flat\" area (the first of flat should be the end of falling limb): negative changes, streamflow varies within threld_flat percentile of change/shifts\n",
    "    obs_gage.loc[:, 'flat'] = (data<=0.01)&(abs(data)<=(np.percentile([abs(x) for x in data if x<0], thrld_flat)))\n",
    "\n",
    "    ### Extract the indices of jump and flat\n",
    "    # indices for 'jumps'\n",
    "    count_dups = [[i, sum(1 for i in group)] for i, group in groupby(obs_gage['jump'])]\n",
    "    results = itertools.accumulate(list(map(operator.itemgetter(1), count_dups)), operator.add)\n",
    "    idx = [each for each in results]\n",
    "\n",
    "    # Add the accumulated index \n",
    "    for i in range(len(count_dups)):\n",
    "        count_dups[i].append(idx[i])\n",
    "\n",
    "    idx_jump = [idx[2]-idx[1] for idx in count_dups if (idx[0]==True)]\n",
    "\n",
    "    # indices for the 'flats'\n",
    "    count_dups = [[i, sum(1 for i in group)] for i, group in groupby(obs_gage['flat'])]\n",
    "    results = itertools.accumulate(list(map(operator.itemgetter(1), count_dups)), operator.add)\n",
    "    idx = [each for each in results]\n",
    "\n",
    "    # Add the accumulated index \n",
    "    for i in range(len(count_dups)):\n",
    "        count_dups[i].append(idx[i])\n",
    "\n",
    "    # flat and decay. both needs to have at least three consecutive flat/deay\n",
    "    idx_flat_start = [idx[2]-idx[1]-1 for idx in count_dups if (idx[0]==True)&(idx[1]>=no_flat)]  #start of flat\n",
    "    idx_flat_end = [idx[2]-1 for idx in count_dups if (idx[0]==True)&(idx[1]>=no_flat)]   # end of flat\n",
    "\n",
    "    obs_gage.loc[:,'flat'] = False\n",
    "    obs_gage.loc[:,'flat'].iloc[idx_flat_start]=True\n",
    "\n",
    "    ### List the event\n",
    "    from datetime import datetime\n",
    "    start = []\n",
    "    end = []\n",
    "\n",
    "    ## Found the first timestep of \"flat\" after a jump\n",
    "    for i in range(len(idx_jump)):\n",
    "        start_tmp = obs_gage.iloc[idx_jump].index[i]\n",
    "        start.append(min(obs_gage.iloc[idx_flat_end].index, key=lambda x: (x>start_tmp, abs(start_tmp-x))))\n",
    "        end.append(min(obs_gage.iloc[idx_flat_start].index, key=lambda x: (x<start_tmp, abs(x-start_tmp))))\n",
    "\n",
    "    ## Create an empty list for storing event info\n",
    "    event_list = pd.DataFrame({\n",
    "                    'start': start,\n",
    "                    'end': end,\n",
    "                    'Qmax': np.nan,\n",
    "                    'Qmin': np.nan,\n",
    "                    'Qrange': np.nan,\n",
    "                    'Qmean': np.nan,\n",
    "                    'Qmedian': np.nan,\n",
    "                    'Qstd': np.nan,\n",
    "                    'Qvar': np.nan,\n",
    "                    'Qskew': np.nan,\n",
    "                    'Qkurt': np.nan,\n",
    "                    'Qtotal': np.nan,\n",
    "                    'Qnpeak': np.nan,\n",
    "                    'Qlen': np.nan,\n",
    "                    'Qtrange': np.nan,\n",
    "                 })\n",
    "    \n",
    "    ## Drop the event that is within another event\n",
    "    event_list = event_list.drop_duplicates(subset='end',keep=\"first\")\n",
    "    event_list = event_list.drop_duplicates(subset='start',keep=\"last\")\n",
    "\n",
    "    ## Extend the event starting/ending time for 3 hour, respectively\n",
    "    event_list.loc[:,'start'] = event_list['start'] - dt.timedelta(hours=3)\n",
    "    event_list.loc[:,'end'] = event_list['end'] + dt.timedelta(hours=3)\n",
    "\n",
    "    ### Discard the event that is shorter than thrld_duration\n",
    "    event_list = event_list[(event_list.end-event_list.start)>=thrld_duration]\n",
    "\n",
    "    ## Mark the event_list back to the obs_gage\n",
    "    obs_gage.loc[:,'event'] = False\n",
    "\n",
    "    for i in range(len(event_list)):\n",
    "        start = event_list.start.iloc[i]\n",
    "        stop = event_list.end.iloc[i]\n",
    "        Qevent = obs_gage.loc[start:stop]\n",
    "        ### filter out if the event max. larger than the threshold\n",
    "        if Qevent.max().value>=thrld_min:\n",
    "\n",
    "            # Calculate event statistics\n",
    "            ## magnitude related\n",
    "            Qmax = Qevent.value.max()    #cms, maximum Q\n",
    "            Qmin = Qevent.value.min()    #cms, minmum Q\n",
    "            Qrange = Qmax - Qmin    #cms, range of Q\n",
    "            Qmean = Qevent.value.mean()    # cms, mean Q\n",
    "            Qmedian = Qevent.value.median()    # cms, median Q\n",
    "            Qstd = Qevent.value.std()    # cms, standard deviation\n",
    "            Qvar = Qevent.value.var()    # cms, variance\n",
    "            Qskew = Qevent.value.skew()    # skewness\n",
    "            Qkurt = Qevent.value.kurt()    # kurt index\n",
    "\n",
    "            Qtotal = Qevent.value.sum()*3600  #m^3, total event discharge\n",
    "            Qnpeak = Qevent.jump.sum() #the number of peaks in the event\n",
    "\n",
    "            ## time related\n",
    "            Qlen = len(Qevent) # event duration (time interval must be \"hour\" here), event length\n",
    "            Qtrange = int((Qevent.loc[Qevent.value==Qmax,'value'].index - start).total_seconds()[0]/3600) - 3 # hour, time to peak (from first Q rise, not starts of event)\n",
    "\n",
    "            ## mark the event\n",
    "            obs_gage.loc[start:stop,'event'] = True\n",
    "\n",
    "            ## write the statistics to the event_list\n",
    "            event_list.iloc[i] = [start, stop, Qmax, Qmin, Qrange, Qmean, Qmedian, Qstd, Qvar, Qskew, Qkurt, Qtotal, Qnpeak, Qlen, Qtrange]\n",
    "\n",
    "    \n",
    "    # Tidy up the dataframes\n",
    "    event_list.dropna(inplace=True)\n",
    "    obs_gage.drop(['jump','flat'], axis=1, inplace=True)\n",
    "    obs_gage.columns = ['Q_cms', 'event']\n",
    "    obs_gage.index.name = 'Datetime'\n",
    "\n",
    "    event_list.to_csv(dir_output + 'event_list_' + gage_id + '.csv', index=False)\n",
    "    obs_gage.to_csv(dir_output + 'obs_event_marked_' + gage_id + '.csv', index=True)\n",
    "\n",
    "    ## Plot\n",
    "    if plot==True:\n",
    "        if plot_start is None:\n",
    "            plot_start = np.nanmin(obs_gage.index)\n",
    "        if plot_end is None:\n",
    "            plot_end = np.nanmax(obs_gage.index)\n",
    "\n",
    "        obs_gage_trim = obs_gage.loc[plot_start:plot_end]\n",
    "\n",
    "        obs_gage_trim.loc[obs_gage_trim.event==False,'event'] = 'blue'\n",
    "        obs_gage_trim.loc[obs_gage_trim.event==True, 'event'] = 'orange'\n",
    "\n",
    "        plt.subplots(figsize=(20,8))\n",
    "        plt.scatter(obs_gage_trim.reset_index()['Datetime'], \n",
    "                    y=obs_gage_trim.reset_index()['Q_cms'], \n",
    "                    c=obs_gage_trim.reset_index()['event'], s=10)\n",
    "\n",
    "        plt.ylabel('Discharge (cms)', labelpad=10, fontsize=24)\n",
    "        plt.xticks(fontsize=16)\n",
    "        plt.yticks(fontsize=16)\n",
    "\n",
    "        plt.savefig(dir_output + 'plot_events_' + gage_id + '.jpg', dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27980f99-fb64-4d7d-b2c3-678070406d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Main code -------------------------------\n",
    "### Read static files\n",
    "site_meta = pd.read_csv(file_site_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f8a8757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16211600, 16213000, 16229000, 16241600, 16249000, 16274100,\n",
       "       16294900, 16301050, 16330000, 16345000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_meta.site_no.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "281d0339-cc3c-4426-b59b-2ac189e058ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve streamflow observations \n",
    "service = IVDataService()\n",
    "observations = service.get(\n",
    "    sites=[16211600, 16213000], \n",
    "    startDT=startDate, \n",
    "    endDT=endDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "783e4a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value_time</th>\n",
       "      <th>variable_name</th>\n",
       "      <th>usgs_site_code</th>\n",
       "      <th>measurement_unit</th>\n",
       "      <th>value</th>\n",
       "      <th>qualifiers</th>\n",
       "      <th>series</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "      <td>streamflow</td>\n",
       "      <td>16211600</td>\n",
       "      <td>ft3/s</td>\n",
       "      <td>0.92</td>\n",
       "      <td>['A']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01 00:15:00</td>\n",
       "      <td>streamflow</td>\n",
       "      <td>16211600</td>\n",
       "      <td>ft3/s</td>\n",
       "      <td>0.83</td>\n",
       "      <td>['A']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01 00:30:00</td>\n",
       "      <td>streamflow</td>\n",
       "      <td>16211600</td>\n",
       "      <td>ft3/s</td>\n",
       "      <td>0.83</td>\n",
       "      <td>['A']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01 00:45:00</td>\n",
       "      <td>streamflow</td>\n",
       "      <td>16211600</td>\n",
       "      <td>ft3/s</td>\n",
       "      <td>0.83</td>\n",
       "      <td>['A']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01 01:00:00</td>\n",
       "      <td>streamflow</td>\n",
       "      <td>16211600</td>\n",
       "      <td>ft3/s</td>\n",
       "      <td>0.83</td>\n",
       "      <td>['A']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137705</th>\n",
       "      <td>2020-12-30 23:40:00</td>\n",
       "      <td>streamflow</td>\n",
       "      <td>16213000</td>\n",
       "      <td>ft3/s</td>\n",
       "      <td>22.40</td>\n",
       "      <td>['A']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137706</th>\n",
       "      <td>2020-12-30 23:45:00</td>\n",
       "      <td>streamflow</td>\n",
       "      <td>16213000</td>\n",
       "      <td>ft3/s</td>\n",
       "      <td>23.00</td>\n",
       "      <td>['A']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137707</th>\n",
       "      <td>2020-12-30 23:50:00</td>\n",
       "      <td>streamflow</td>\n",
       "      <td>16213000</td>\n",
       "      <td>ft3/s</td>\n",
       "      <td>23.00</td>\n",
       "      <td>['A']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137708</th>\n",
       "      <td>2020-12-30 23:55:00</td>\n",
       "      <td>streamflow</td>\n",
       "      <td>16213000</td>\n",
       "      <td>ft3/s</td>\n",
       "      <td>23.00</td>\n",
       "      <td>['A']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137709</th>\n",
       "      <td>2020-12-31 00:00:00</td>\n",
       "      <td>streamflow</td>\n",
       "      <td>16213000</td>\n",
       "      <td>ft3/s</td>\n",
       "      <td>22.40</td>\n",
       "      <td>['A']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137710 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                value_time variable_name usgs_site_code measurement_unit  \\\n",
       "0      2020-01-01 00:00:00    streamflow       16211600            ft3/s   \n",
       "1      2020-01-01 00:15:00    streamflow       16211600            ft3/s   \n",
       "2      2020-01-01 00:30:00    streamflow       16211600            ft3/s   \n",
       "3      2020-01-01 00:45:00    streamflow       16211600            ft3/s   \n",
       "4      2020-01-01 01:00:00    streamflow       16211600            ft3/s   \n",
       "...                    ...           ...            ...              ...   \n",
       "137705 2020-12-30 23:40:00    streamflow       16213000            ft3/s   \n",
       "137706 2020-12-30 23:45:00    streamflow       16213000            ft3/s   \n",
       "137707 2020-12-30 23:50:00    streamflow       16213000            ft3/s   \n",
       "137708 2020-12-30 23:55:00    streamflow       16213000            ft3/s   \n",
       "137709 2020-12-31 00:00:00    streamflow       16213000            ft3/s   \n",
       "\n",
       "        value qualifiers series  \n",
       "0        0.92      ['A']      0  \n",
       "1        0.83      ['A']      0  \n",
       "2        0.83      ['A']      0  \n",
       "3        0.83      ['A']      0  \n",
       "4        0.83      ['A']      0  \n",
       "...       ...        ...    ...  \n",
       "137705  22.40      ['A']      0  \n",
       "137706  23.00      ['A']      0  \n",
       "137707  23.00      ['A']      0  \n",
       "137708  23.00      ['A']      0  \n",
       "137709  22.40      ['A']      0  \n",
       "\n",
       "[137710 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58a40314-1ab5-48c8-ad19-2fd900cf23d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pn/p22nfm7d03l25m6v68vhscv80000gn/T/ipykernel_951/3352395016.py:14: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  obs = obs.groupby([\n"
     ]
    }
   ],
   "source": [
    "# Drop extra columns to be more efficient\n",
    "obs = observations[[\n",
    "    'usgs_site_code', \n",
    "    'value_time', \n",
    "    'value'\n",
    "    ]]\n",
    "\n",
    "# Check for duplicate time series, keep first by default\n",
    "obs = obs.drop_duplicates(\n",
    "    subset=['usgs_site_code', 'value_time']\n",
    "    )\n",
    "\n",
    "# Resample to hourly, keep first measurement in each 15-min bin\n",
    "obs = obs.groupby([\n",
    "    'usgs_site_code',\n",
    "    Grouper(key='value_time', freq='1H')\n",
    "    ]).first() ### Note, ffill will also fill those no-data.\n",
    "\n",
    "# convert cfs to cms\n",
    "obs = obs*cfs2cms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e3667ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usgs_site_code</th>\n",
       "      <th>value_time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">16211600</th>\n",
       "      <th>2020-01-01 00:00:00</th>\n",
       "      <td>0.026054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:00:00</th>\n",
       "      <td>0.023506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 02:00:00</th>\n",
       "      <td>0.023506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 03:00:00</th>\n",
       "      <td>0.023506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 04:00:00</th>\n",
       "      <td>0.023506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">16213000</th>\n",
       "      <th>2020-12-30 20:00:00</th>\n",
       "      <td>0.651360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-30 21:00:00</th>\n",
       "      <td>0.651360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-30 22:00:00</th>\n",
       "      <td>0.651360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-30 23:00:00</th>\n",
       "      <td>0.634368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31 00:00:00</th>\n",
       "      <td>0.634368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17522 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       value\n",
       "usgs_site_code value_time                   \n",
       "16211600       2020-01-01 00:00:00  0.026054\n",
       "               2020-01-01 01:00:00  0.023506\n",
       "               2020-01-01 02:00:00  0.023506\n",
       "               2020-01-01 03:00:00  0.023506\n",
       "               2020-01-01 04:00:00  0.023506\n",
       "...                                      ...\n",
       "16213000       2020-12-30 20:00:00  0.651360\n",
       "               2020-12-30 21:00:00  0.651360\n",
       "               2020-12-30 22:00:00  0.651360\n",
       "               2020-12-30 23:00:00  0.634368\n",
       "               2020-12-31 00:00:00  0.634368\n",
       "\n",
       "[17522 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "e3a49ab8-5add-4e5f-a316-9835c3699897",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/lib/python3.7/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "## identify hydrological event \n",
    "# gage = site_meta.site_no[0]\n",
    "# gage_id = str(gage)\n",
    "# obs_gage = obs.loc[gage_id].copy()\n",
    "# obs_gage = obs_gage.tz_localize('UTC')\n",
    "### Manually fix the issue of the Waikele gage flown away =============================\n",
    "gage = site_meta.site_no[1]\n",
    "gage_id = str(gage)\n",
    "obs_gage = obs.loc[gage_id].copy()\n",
    "obs_gage.loc[(obs_gage.index >= dt.datetime(2016,7,22)) &\n",
    "         (obs_gage.index <= dt.datetime(2016,7,29)) &\n",
    "         (obs_gage.value > 60)] = np.nan\n",
    "obs_gage = obs_gage.tz_localize('UTC')\n",
    "### ===================================================================================\n",
    "dir_output = dir_result\n",
    "thrld_min = site_meta.loc[site_meta.site_no==gage, 'Q5_cms'].values[0]\n",
    "thrld_jump = 80\n",
    "thrld_flat = 75  # original was 50\n",
    "\n",
    "event_identify(obs_gage, dir_output, gage_id, thrld_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "22454117-7d9e-42b2-b191-daaad2c17da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/lib/python3.7/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "/opt/miniconda/lib/python3.7/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "/opt/miniconda/lib/python3.7/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "/opt/miniconda/lib/python3.7/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "/opt/miniconda/lib/python3.7/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "/opt/miniconda/lib/python3.7/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "/opt/miniconda/lib/python3.7/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "/opt/miniconda/lib/python3.7/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "## identify hydrological event\n",
    "for i in range(2,10):\n",
    "    gage = site_meta.site_no[i]\n",
    "    gage_id = str(gage)\n",
    "    obs_gage = obs.loc[gage_id].copy()\n",
    "    obs_gage = obs_gage.tz_localize('UTC')\n",
    "    dir_output = dir_result\n",
    "    thrld_min = site_meta.loc[site_meta.site_no==gage, 'Q5_cms'].values[0]\n",
    "    thrld_jump = 80\n",
    "    thrld_flat = 75  # original was 50\n",
    "\n",
    "    event_identify(obs_gage, dir_output, gage_id, thrld_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8501cfed-92fb-4fa5-8387-ca28fc3a029e",
   "metadata": {},
   "source": [
    "### Event statistics for each watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "22ed9112-645a-4f61-bcd5-4c01eab04b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_meta.loc[:,'drain_area_va']=site_meta['drain_area_va']*sqmi2sqkm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "0fe38659-eaef-4e75-ac84-48cdf1927fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_stat = pd.DataFrame({\n",
    "                'site_no': site_meta.site_no,\n",
    "                'site_name': site_meta.station_nm.values,\n",
    "                'nQmax_mean': np.nan,\n",
    "                'nQmin_mean': np.nan,\n",
    "                'nQrange_mean': np.nan,\n",
    "                'nQmean_mean': np.nan,\n",
    "                'nQmedian_mean': np.nan,\n",
    "                'nQstd_mean': np.nan,\n",
    "                'nQvar_mean': np.nan,\n",
    "                'Qskew_mean': np.nan,\n",
    "                'Qkurt_mean': np.nan,\n",
    "                'Qtotal_per_hr': np.nan,\n",
    "                'Qnpeak_mean': np.nan,\n",
    "                'Qlen_mean': np.nan,\n",
    "                'Qtrange_mean': np.nan,\n",
    "                'count': np.nan\n",
    "})\n",
    "\n",
    "event_stat.set_index(['site_no'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "a0db0465-6a4a-4aed-b68b-b4e691628d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gage = site_meta.site_no[0]\n",
    "\n",
    "i=0\n",
    "for gage in site_meta.site_no:\n",
    "    event = pd.read_csv(dir_result + 'event_list_' + str(gage) + '.csv', index_col=0)\n",
    "    drain_area = site_meta.drain_area_va[site_meta.site_no==gage].values[0]\n",
    "    event.iloc[:,2:9] = event.iloc[:,2:9]/drain_area\n",
    "    event_stat.iloc[i,1:14] = event.mean(numeric_only=True).values\n",
    "    event_stat.loc[gage,'Qtotal_per_hr'] = event.Qtotal.sum()/event.Qlen.sum()\n",
    "    event_stat.loc[gage,'count'] = len(event)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "8add702f-4b03-4d1a-943d-893a5d2a07c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_stat.to_csv(dir_result + 'event_list_summary.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
